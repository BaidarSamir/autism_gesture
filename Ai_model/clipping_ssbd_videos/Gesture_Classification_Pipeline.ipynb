{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937201e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3221d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33debac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243a062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cpu\n",
      "âœ… Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Complete Autism Gesture Detection System\n",
    "# Goal: Detect autism-related gestures (ArmFlapping, HeadBanging, Spinning) from any length video\n",
    "# Approach: 3 binary classifiers + ensemble for final decision\n",
    "# Cell 1: Environment Setup and Library Imports\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'DATA_FOLDER': '/home/samir/projects/clipping_ssbd_videos/ssbd_clip_segment/',\n",
    "    'SEQUENCE_LENGTH': 16,  # Reduced for better performance\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'BATCH_SIZE': 8,\n",
    "    'LEARNING_RATE': 0.0001,\n",
    "    'EPOCHS': 40,\n",
    "    'PATIENCE': 10,  # Early stopping\n",
    "    'GESTURE_NAMES': ['ArmFlapping', 'HeadBanging', 'Spinning']\n",
    "}\n",
    "\n",
    "print(\"âœ… Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e50bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Advanced Video Dataset with Negative Sample Generation\n",
    "class AutismGestureDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, gesture_type=None, transform=None, \n",
    "                 sequence_length=16, img_size=(224, 224), mode='train'):\n",
    "        \"\"\"\n",
    "        Dataset for binary classification of specific autism gesture\n",
    "        Args:\n",
    "            video_paths: List of video paths\n",
    "            labels: Binary labels (1: has gesture, 0: no gesture)\n",
    "            gesture_type: Specific gesture to detect (0: ArmFlapping, 1: HeadBanging, 2: Spinning)\n",
    "            transform: Image transformations\n",
    "            sequence_length: Number of frames to extract\n",
    "            img_size: Target image size\n",
    "            mode: 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.gesture_type = gesture_type\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def extract_keyframes(self, video_path):\n",
    "        \"\"\"Extract key frames using optical flow and motion analysis\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        motion_scores = []\n",
    "        \n",
    "        # Read all frames first\n",
    "        all_frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_resized = cv2.resize(frame_rgb, self.img_size)\n",
    "            all_frames.append(frame_resized)\n",
    "        cap.release()\n",
    "        \n",
    "        if len(all_frames) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Calculate motion between consecutive frames\n",
    "        for i in range(1, len(all_frames)):\n",
    "            frame1_gray = cv2.cvtColor(all_frames[i-1], cv2.COLOR_RGB2GRAY)\n",
    "            frame2_gray = cv2.cvtColor(all_frames[i], cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Calculate optical flow\n",
    "            try:\n",
    "                flow = cv2.calcOpticalFlowPyrLK(\n",
    "                    frame1_gray, frame2_gray, \n",
    "                    np.array([[x, y] for x in range(0, frame1_gray.shape[1], 20) \n",
    "                             for y in range(0, frame1_gray.shape[0], 20)], dtype=np.float32).reshape(-1, 1, 2),\n",
    "                    None\n",
    "                )[0]\n",
    "                # Calculate motion magnitude\n",
    "                motion = np.mean(np.sqrt(np.sum(flow**2, axis=2))) if flow is not None else 0\n",
    "            except:\n",
    "                motion = 0\n",
    "            motion_scores.append(motion)\n",
    "        \n",
    "        # Select frames based on motion and uniform sampling\n",
    "        if len(all_frames) <= self.sequence_length:\n",
    "            # For short videos: repeat frames\n",
    "            selected_frames = all_frames.copy()\n",
    "            while len(selected_frames) < self.sequence_length:\n",
    "                selected_frames.extend(all_frames[:min(len(all_frames), \n",
    "                                                     self.sequence_length - len(selected_frames))])\n",
    "            frames = selected_frames[:self.sequence_length]\n",
    "        else:\n",
    "            # For longer videos: smart sampling\n",
    "            if self.mode == 'train' and random.random() < 0.3:\n",
    "                # Random sampling for augmentation\n",
    "                indices = sorted(random.sample(range(len(all_frames)), self.sequence_length))\n",
    "            else:\n",
    "                # Motion-based + uniform sampling\n",
    "                if len(motion_scores) > 0:\n",
    "                    high_motion_indices = np.argsort(motion_scores)[-self.sequence_length//2:]\n",
    "                    uniform_indices = np.linspace(0, len(all_frames)-1, self.sequence_length//2, dtype=int)\n",
    "                    combined_indices = sorted(set(list(high_motion_indices) + list(uniform_indices)))\n",
    "                    if len(combined_indices) >= self.sequence_length:\n",
    "                        indices = combined_indices[:self.sequence_length]\n",
    "                    else:\n",
    "                        # Fill remaining with uniform sampling\n",
    "                        remaining = self.sequence_length - len(combined_indices)\n",
    "                        extra_indices = np.linspace(0, len(all_frames)-1, remaining, dtype=int)\n",
    "                        indices = sorted(set(list(combined_indices) + list(extra_indices)))[:self.sequence_length]\n",
    "                else:\n",
    "                    # Fallback to uniform sampling\n",
    "                    indices = np.linspace(0, len(all_frames)-1, self.sequence_length, dtype=int)\n",
    "            frames = [all_frames[i] for i in indices]\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self.extract_keyframes(video_path)\n",
    "        \n",
    "        if frames is None:\n",
    "            # Return dummy data if video loading fails\n",
    "            frames = [np.zeros((*self.img_size, 3), dtype=np.uint8) for _ in range(self.sequence_length)]\n",
    "        \n",
    "        # Apply transforms\n",
    "        # Apply transforms\n",
    "        transformed_frames = []\n",
    "        for frame in frames:\n",
    "            frame_pil = Image.fromarray(frame.astype('uint8'))\n",
    "            transformed_frame = self.transform(frame_pil)\n",
    "            transformed_frames.append(transformed_frame)\n",
    "\n",
    "        # Ensure fixed-length sequence\n",
    "        while len(transformed_frames) < self.sequence_length:\n",
    "            transformed_frames.append(transformed_frames[-1])  # repeat last frame\n",
    "\n",
    "        transformed_frames = transformed_frames[:self.sequence_length]  # truncate if too long\n",
    "\n",
    "        video_tensor = torch.stack(transformed_frames)\n",
    "        \n",
    "        return video_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b13a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Binary Gesture Classifier with Transfer Learning\n",
    "class BinaryGestureClassifier(nn.Module):\n",
    "    def __init__(self, sequence_length=16, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Binary classifier for specific autism gesture using transfer learning\n",
    "        \"\"\"\n",
    "        super(BinaryGestureClassifier, self).__init__()\n",
    "        \n",
    "        # Pre-trained ResNet18 as feature extractor\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()  # Remove final layer\n",
    "        \n",
    "        # Freeze early layers, fine-tune later layers\n",
    "        for param in list(self.backbone.parameters())[:-10]:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.feature_dim = 512\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Temporal modeling - simplified LSTM\n",
    "        self.temporal = nn.LSTM(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for important frame selection\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(256, 64),  # 256 from bidirectional LSTM\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)  # Binary classification\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.shape\n",
    "        \n",
    "        # Extract features for each frame\n",
    "        x = x.view(batch_size * seq_len, C, H, W)\n",
    "        features = self.backbone(x)  # (batch_size * seq_len, 512)\n",
    "        features = features.view(batch_size, seq_len, self.feature_dim)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        lstm_out, _ = self.temporal(features)  # (batch_size, seq_len, 256)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(lstm_out)  # (batch_size, seq_len, 1)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Weighted average of LSTM outputs\n",
    "        attended_features = torch.sum(lstm_out * attention_weights, dim=1)  # (batch_size, 256)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(attended_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f1af87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Loading and Preprocessing Functions\n",
    "def load_and_prepare_data(data_folder):\n",
    "    \"\"\"\n",
    "    Load dataset and prepare for binary classification training\n",
    "    \"\"\"\n",
    "    gesture_names = CONFIG['GESTURE_NAMES']\n",
    "    \n",
    "    # Load original data\n",
    "    video_paths = []\n",
    "    original_labels = []\n",
    "    \n",
    "    for idx, gesture_name in enumerate(gesture_names):\n",
    "        gesture_folder = os.path.join(data_folder, gesture_name)\n",
    "        if not os.path.exists(gesture_folder):\n",
    "            print(f\"âŒ Warning: {gesture_folder} not found!\")\n",
    "            continue\n",
    "        videos = glob.glob(os.path.join(gesture_folder, '*.avi'))\n",
    "        print(f\"ðŸ“ {gesture_name}: {len(videos)} videos\")\n",
    "        video_paths.extend(videos)\n",
    "        original_labels.extend([idx] * len(videos))\n",
    "        \n",
    "    print(f\"ðŸ“Š Total videos: {len(video_paths)}\")\n",
    "    return video_paths, original_labels, gesture_names\n",
    "\n",
    "def create_binary_datasets(video_paths, original_labels, target_gesture_idx):\n",
    "    \"\"\"\n",
    "    Create binary dataset for specific gesture detection\n",
    "    Args:\n",
    "        video_paths: List of video paths\n",
    "        original_labels: Original multi-class labels\n",
    "        target_gesture_idx: Index of target gesture (0, 1, or 2)\n",
    "    Returns:\n",
    "        Binary labeled dataset\n",
    "    \"\"\"\n",
    "    binary_labels = [1 if label == target_gesture_idx else 0 for label in original_labels]\n",
    "    \n",
    "    # Count positive and negative samples\n",
    "    pos_count = sum(binary_labels)\n",
    "    neg_count = len(binary_labels) - pos_count\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Gesture {CONFIG['GESTURE_NAMES'][target_gesture_idx]}:\")\n",
    "    print(f\"   âœ… Positive samples: {pos_count}\")\n",
    "    print(f\"   âŒ Negative samples: {neg_count}\")\n",
    "    \n",
    "    return video_paths, binary_labels\n",
    "\n",
    "def create_data_loaders(video_paths, labels, target_gesture_idx, test_size=0.2, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Create data loaders with proper stratification and augmentation\n",
    "    \"\"\"\n",
    "    # Data augmentation transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomRotation(degrees=5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Stratified split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        video_paths, labels, test_size=test_size, stratify=labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Split sizes - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AutismGestureDataset(\n",
    "        X_train, y_train, target_gesture_idx, train_transform, \n",
    "        CONFIG['SEQUENCE_LENGTH'], CONFIG['IMG_SIZE'], 'train'\n",
    "    )\n",
    "    \n",
    "    val_dataset = AutismGestureDataset(\n",
    "        X_val, y_val, target_gesture_idx, val_test_transform,\n",
    "        CONFIG['SEQUENCE_LENGTH'], CONFIG['IMG_SIZE'], 'val'\n",
    "    )\n",
    "    \n",
    "    test_dataset = AutismGestureDataset(\n",
    "        X_test, y_test, target_gesture_idx, val_test_transform,\n",
    "        CONFIG['SEQUENCE_LENGTH'], CONFIG['IMG_SIZE'], 'test'\n",
    "    )\n",
    "    \n",
    "    # Weighted sampling for imbalanced classes\n",
    "    class_counts = Counter(y_train)\n",
    "    class_weights = {cls: 1.0/count for cls, count in class_counts.items()}\n",
    "    sample_weights = [class_weights[label] for label in y_train]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], \n",
    "                             sampler=sampler, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], \n",
    "                           shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'], \n",
    "                            shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44ea8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Function with Early Stopping and Advanced Metrics\n",
    "def train_binary_classifier(model, train_loader, val_loader, gesture_name, epochs=40):\n",
    "    \"\"\"\n",
    "    Train binary classifier with advanced monitoring\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n",
    "                                                    patience=5, )\n",
    "    \n",
    "    # Tracking variables\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    train_aucs, val_aucs = [], []\n",
    "    best_val_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"ðŸš€ Training {gesture_name} classifier...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds, train_targets = [], []\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        \n",
    "        for videos, labels in train_pbar:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().detach().numpy())\n",
    "            train_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_acc = accuracy_score(train_targets, np.array(train_preds) > 0.5)\n",
    "            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}', 'Acc': f'{current_acc:.3f}'})\n",
    "            \n",
    "        # Calculate training metrics\n",
    "        train_acc = accuracy_score(train_targets, np.array(train_preds) > 0.5)\n",
    "        train_auc = roc_auc_score(train_targets, train_preds)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for videos, labels in val_loader:\n",
    "                videos, labels = videos.to(device), labels.to(device)\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "                \n",
    "        # Calculate validation metrics\n",
    "        val_acc = accuracy_score(val_targets, np.array(val_preds) > 0.5)\n",
    "        val_auc = roc_auc_score(val_targets, val_preds)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_aucs.append(train_auc)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.3f}, AUC: {train_auc:.3f}')\n",
    "        print(f'  Val   - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.3f}, AUC: {val_auc:.3f}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f'  ðŸŽ‰ New best AUC: {best_val_auc:.3f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= CONFIG['PATIENCE']:\n",
    "            print(f'â¹ï¸ Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "            \n",
    "        print('-' * 60)\n",
    "        \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f'âœ… Best model loaded with AUC: {best_val_auc:.3f}')\n",
    "        \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'train_aucs': train_aucs,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_auc': best_val_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82fbc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Evaluation and Visualization Functions\n",
    "def evaluate_binary_classifier(model, test_loader, gesture_name):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of binary classifier\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(test_loader, desc=f'Testing {gesture_name}'):\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            outputs = model(videos)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            probabilities.extend(probs[:, 1].cpu().numpy())  # Probability of positive class\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    auc_score = roc_auc_score(true_labels, probabilities)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {gesture_name} Classifier Results:\")\n",
    "    print(f\"  ðŸŽ¯ Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  ðŸ“ˆ AUC Score: {auc_score:.3f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nðŸ“‹ Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, \n",
    "                              target_names=['No Gesture', gesture_name]))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'auc_score': auc_score,\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "def plot_training_history(history, gesture_name):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training history\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{gesture_name} Training History', fontsize=16)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss', color='red')\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0, 1].plot(history['train_accs'], label='Train Acc', color='blue')\n",
    "    axes[0, 1].plot(history['val_accs'], label='Val Acc', color='red')\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # AUC plot\n",
    "    axes[1, 0].plot(history['train_aucs'], label='Train AUC', color='blue')\n",
    "    axes[1, 0].plot(history['val_aucs'], label='Val AUC', color='red')\n",
    "    axes[1, 0].set_title('AUC Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('AUC')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # ROC Curve (placeholder for test data)\n",
    "    axes[1, 1].text(0.5, 0.5, 'ROC Curve\\n(Run evaluation first)', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('ROC Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(results, gesture_name):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for binary classifier\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(results['true_labels'], results['probabilities'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{gesture_name} (AUC = {results[\"auc_score\"]:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {gesture_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e9572c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main Training Pipeline\n",
    "def main_training_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline to train all three binary classifiers\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting Autism Gesture Detection Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"ðŸ“‚ Loading dataset...\")\n",
    "    video_paths, original_labels, gesture_names = load_and_prepare_data(CONFIG['DATA_FOLDER'])\n",
    "    \n",
    "    # Store trained models and results\n",
    "    trained_models = {}\n",
    "    training_histories = {}\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    # Train binary classifier for each gesture\n",
    "    for gesture_idx, gesture_name in enumerate(gesture_names):\n",
    "        print(f\"\\nðŸŽ¯ Training Binary Classifier for: {gesture_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Prepare binary dataset\n",
    "        binary_video_paths, binary_labels = create_binary_datasets(\n",
    "            video_paths, original_labels, gesture_idx\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(\n",
    "            binary_video_paths, binary_labels, gesture_idx\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = BinaryGestureClassifier(\n",
    "            sequence_length=CONFIG['SEQUENCE_LENGTH']\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"ðŸ§  Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "        # Train model\n",
    "        trained_model, history = train_binary_classifier(\n",
    "            model, train_loader, val_loader, gesture_name, CONFIG['EPOCHS']\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_binary_classifier(trained_model, test_loader, gesture_name)\n",
    "        \n",
    "        # Store results\n",
    "        trained_models[gesture_name] = trained_model\n",
    "        training_histories[gesture_name] = history\n",
    "        evaluation_results[gesture_name] = results\n",
    "        \n",
    "        # Plot training history\n",
    "        plot_training_history(history, gesture_name)\n",
    "        plot_roc_curve(results, gesture_name)\n",
    "        \n",
    "        # Save individual model\n",
    "        model_path = f'binary_classifier_{gesture_name.lower()}.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'model_config': {\n",
    "                'sequence_length': CONFIG['SEQUENCE_LENGTH'],\n",
    "                'gesture_name': gesture_name,\n",
    "                'gesture_idx': gesture_idx\n",
    "            },\n",
    "            'training_history': history,\n",
    "            'evaluation_results': results\n",
    "        }, model_path)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Model saved: {model_path}\")\n",
    "        print(f\"âœ… {gesture_name} classifier training complete!\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    return trained_models, training_histories, evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c343078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Ensemble Prediction System for Any Length Video\n",
    "class AutismGestureEnsemble:\n",
    "    def __init__(self, models_dict, gesture_names, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Ensemble system combining all three binary classifiers\n",
    "        Args:\n",
    "            models_dict: Dictionary of trained models {gesture_name: model}\n",
    "            gesture_names: List of gesture names\n",
    "            threshold: Confidence threshold for positive detection\n",
    "        \"\"\"\n",
    "        self.models = models_dict\n",
    "        self.gesture_names = gesture_names\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Set all models to evaluation mode\n",
    "        for model in self.models.values():\n",
    "            model.eval()\n",
    "            \n",
    "    def predict_single_video(self, video_path, return_details=False):\n",
    "        \"\"\"\n",
    "        Predict autism gestures for a single video of any length\n",
    "        Args:\n",
    "            video_path: Path to video file\n",
    "            return_details: If True, return detailed probabilities\n",
    "        Returns:\n",
    "            prediction: Dict with detected gestures and confidences\n",
    "        \"\"\"\n",
    "        # Preprocessing transform\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Create temporary dataset for single video\n",
    "        temp_dataset = AutismGestureDataset(\n",
    "            [video_path], [0], transform=transform,\n",
    "            sequence_length=CONFIG['SEQUENCE_LENGTH'],\n",
    "            img_size=CONFIG['IMG_SIZE'], mode='test'\n",
    "        )\n",
    "        \n",
    "        # Get video tensor\n",
    "        video_tensor, _ = temp_dataset[0]\n",
    "        video_tensor = video_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        results = {}\n",
    "        detected_gestures = []\n",
    "        \n",
    "        # Test each binary classifier\n",
    "        with torch.no_grad():\n",
    "            for gesture_name, model in self.models.items():\n",
    "                outputs = model(video_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence = probabilities[0, 1].item()  # Probability of positive class\n",
    "                \n",
    "                results[gesture_name] = {\n",
    "                    'confidence': confidence,\n",
    "                    'detected': confidence > self.threshold\n",
    "                }\n",
    "                \n",
    "                if confidence > self.threshold:\n",
    "                    detected_gestures.append({\n",
    "                        'gesture': gesture_name,\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "                    \n",
    "        # Sort by confidence\n",
    "        detected_gestures.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        # Overall prediction\n",
    "        prediction = {\n",
    "            'has_autism_gesture': len(detected_gestures) > 0,\n",
    "            'detected_gestures': detected_gestures,\n",
    "            'primary_gesture': detected_gestures[0]['gesture'] if detected_gestures else None,\n",
    "            'max_confidence': detected_gestures[0]['confidence'] if detected_gestures else 0.0,\n",
    "            'all_confidences': results\n",
    "        }\n",
    "        \n",
    "        if return_details:\n",
    "            prediction['video_path'] = video_path\n",
    "            prediction['threshold'] = self.threshold\n",
    "            \n",
    "        return prediction\n",
    "    \n",
    "    def predict_batch_videos(self, video_paths, batch_size=4):\n",
    "        \"\"\"\n",
    "        Predict autism gestures for multiple videos\n",
    "        Args:\n",
    "            video_paths: List of video paths\n",
    "            batch_size: Processing batch size\n",
    "        Returns:\n",
    "            List of predictions for each video\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        print(f\"ðŸ”„ Processing {len(video_paths)} videos...\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(video_paths), batch_size)):\n",
    "            batch_paths = video_paths[i:i+batch_size]\n",
    "            \n",
    "            for video_path in batch_paths:\n",
    "                try:\n",
    "                    prediction = self.predict_single_video(video_path)\n",
    "                    predictions.append(prediction)\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error processing {video_path}: {str(e)}\")\n",
    "                    predictions.append({\n",
    "                        'has_autism_gesture': False,\n",
    "                        'detected_gestures': [],\n",
    "                        'primary_gesture': None,\n",
    "                        'max_confidence': 0.0,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                    \n",
    "        return predictions\n",
    "    \n",
    "    def set_threshold(self, new_threshold):\n",
    "        \"\"\"Update confidence threshold\"\"\"\n",
    "        self.threshold = new_threshold\n",
    "        print(f\"ðŸŽ¯ Threshold updated to: {new_threshold}\")\n",
    "aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24ff147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model Loading and Saving Functions\n",
    "def save_ensemble_model(trained_models, save_path='autism_gesture_ensemble.pth'):\n",
    "    \"\"\"\n",
    "    Save the complete ensemble model\n",
    "    Args:\n",
    "        trained_models: Dictionary of trained models\n",
    "        save_path: Path to save the ensemble\n",
    "    \"\"\"\n",
    "    ensemble_data = {\n",
    "        'model_states': {},\n",
    "        'gesture_names': CONFIG['GESTURE_NAMES'],\n",
    "        'config': CONFIG,\n",
    "        'timestamp': torch.tensor(cv2.getTickCount())\n",
    "    }\n",
    "    \n",
    "    for gesture_name, model in trained_models.items():\n",
    "        ensemble_data['model_states'][gesture_name] = model.state_dict()\n",
    "        \n",
    "    torch.save(ensemble_data, save_path)\n",
    "    print(f\"ðŸ’¾ Ensemble model saved: {save_path}\")\n",
    "\n",
    "def load_ensemble_model(model_path='autism_gesture_ensemble.pth', threshold=0.5):\n",
    "    \"\"\"\n",
    "    Load complete ensemble model\n",
    "    Args:\n",
    "        model_path: Path to saved ensemble model\n",
    "        threshold: Detection threshold\n",
    "    Returns:\n",
    "        AutismGestureEnsemble instance\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"âŒ Model file not found: {model_path}\")\n",
    "        return None\n",
    "        \n",
    "    # Load saved data\n",
    "    ensemble_data = torch.load(model_path, map_location=device)\n",
    "    gesture_names = ensemble_data['gesture_names']\n",
    "    \n",
    "    # Reconstruct models\n",
    "    models_dict = {}\n",
    "    \n",
    "    for gesture_name in gesture_names:\n",
    "        model = BinaryGestureClassifier(\n",
    "            sequence_length=ensemble_data['config']['SEQUENCE_LENGTH']\n",
    "        ).to(device)\n",
    "        \n",
    "        model.load_state_dict(ensemble_data['model_states'][gesture_name])\n",
    "        model.eval()\n",
    "        models_dict[gesture_name] = model\n",
    "        print(f\"âœ… Loaded {gesture_name} classifier\")\n",
    "        \n",
    "    # Create ensemble\n",
    "    ensemble = AutismGestureEnsemble(models_dict, gesture_names, threshold)\n",
    "    print(f\"âœ… Ensemble model loaded: {model_path}\")\n",
    "    \n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Export Models for Deployment\n",
    "def export_models_for_deployment(trained_models, export_dir='exported_models'):\n",
    "    \"\"\"\n",
    "    Export trained models in various formats for deployment\n",
    "    Args:\n",
    "        trained_models: Dictionary of trained models\n",
    "        export_dir: Directory to save exported models\n",
    "    \"\"\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    print(\"ðŸ“¦ Exporting models for deployment...\")\n",
    "    \n",
    "    # Create dummy input for tracing\n",
    "    dummy_input = torch.randn(1, CONFIG['SEQUENCE_LENGTH'], 3, 224, 224).to(device)\n",
    "    \n",
    "    for gesture_name, model in trained_models.items():\n",
    "        model.eval()\n",
    "        \n",
    "        # TorchScript export\n",
    "        try:\n",
    "            traced_model = torch.jit.trace(model, dummy_input)\n",
    "            torchscript_path = os.path.join(export_dir, f'{gesture_name.lower()}_torchscript.pt')\n",
    "            traced_model.save(torchscript_path)\n",
    "            print(f\"âœ… TorchScript saved: {torchscript_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TorchScript export failed for {gesture_name}: {str(e)}\")\n",
    "            \n",
    "        # ONNX export\n",
    "        try:\n",
    "            onnx_path = os.path.join(export_dir, f'{gesture_name.lower()}_model.onnx')\n",
    "            torch.onnx.export(\n",
    "                model, dummy_input, onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input'],\n",
    "                output_names=['output'],\n",
    "                dynamic_axes={\n",
    "                    'input': {0: 'batch_size'},\n",
    "                    'output': {0: 'batch_size'}\n",
    "                }\n",
    "            )\n",
    "            print(f\"âœ… ONNX saved: {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ONNX export failed for {gesture_name}: {str(e)}\")\n",
    "            \n",
    "    # Save ensemble configuration\n",
    "    config_path = os.path.join(export_dir, 'ensemble_config.json')\n",
    "    config_data = {\n",
    "        'gesture_names': CONFIG['GESTURE_NAMES'],\n",
    "        'sequence_length': CONFIG['SEQUENCE_LENGTH'],\n",
    "        'img_size': CONFIG['IMG_SIZE'],\n",
    "        'model_architecture': 'BinaryGestureClassifier',\n",
    "        'input_shape': [CONFIG['SEQUENCE_LENGTH'], 3, 224, 224],\n",
    "        'output_shape': [2],\n",
    "        'preprocessing': {\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'resize': [224, 224]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "        \n",
    "    print(f\"âœ… Configuration saved: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Welcome to Autism Gesture Detection System\n",
      "============================================================\n",
      "âœ… Loaded ArmFlapping classifier\n",
      "âœ… Loaded HeadBanging classifier\n",
      "âœ… Loaded Spinning classifier\n",
      "âœ… Ensemble model loaded: autism_gesture_ensemble.pth\n",
      "ðŸ¤– Starting Autism Gesture Detection CLI Interface\n",
      "============================================================\n",
      "\n",
      "MENU:\n",
      "1. Test single video\n",
      "2. Exit\n",
      "ðŸ” Analyzing video: \\\\wsl.localhost\\Ubuntu\\home\\samir\\projects\\clipping_ssbd_videos\\ssbd_raw\\v_Spinning_07.mp4\n",
      "\n",
      "ðŸ“Š Analysis Results:\n",
      "ðŸŽ¯ Primary Gesture: Spinning\n",
      "ðŸ“ˆ Confidence: 0.958\n",
      "\n",
      "ðŸ“‹ Detailed Confidences:\n",
      "  ArmFlapping: 0.037\n",
      "  HeadBanging: 0.013\n",
      "  Spinning: 0.958\n",
      "\n",
      "âœ… Autism-related gesture detected!\n",
      "\n",
      "MENU:\n",
      "1. Test single video\n",
      "2. Exit\n",
      "ðŸ” Analyzing video: C:\\Users\\Lenovo\\Downloads\\CRkgy266nVA.mp4\n",
      "\n",
      "ðŸ“Š Analysis Results:\n",
      "ðŸŽ¯ Primary Gesture: None\n",
      "ðŸ“ˆ Confidence: 0.000\n",
      "\n",
      "ðŸ“‹ Detailed Confidences:\n",
      "  ArmFlapping: 0.319\n",
      "  HeadBanging: 0.064\n",
      "  Spinning: 0.219\n",
      "\n",
      "ðŸš« No autism-related gesture detected.\n",
      "\n",
      "MENU:\n",
      "1. Test single video\n",
      "2. Exit\n",
      "ðŸ‘‹ Exiting CLI interface. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Complete Demo and Testing Functions\n",
    "\n",
    "def run_complete_demo():\n",
    "    \"\"\"\n",
    "    Complete demonstration of the autism gesture detection system\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ¬ Starting Complete Autism Gesture Detection Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Train models (if not already trained)\n",
    "    print(\"ðŸš€ Step 1: Training Models\")\n",
    "    trained_models, training_histories, evaluation_results = main_training_pipeline()\n",
    "    \n",
    "    # Step 2: Save ensemble model\n",
    "    print(\"\\nðŸ’¾ Step 2: Saving Ensemble Model\")\n",
    "    save_ensemble_model(trained_models)\n",
    "    \n",
    "    # Step 3: Export for deployment\n",
    "    print(\"\\nðŸ“¦ Step 3: Exporting for Deployment\")\n",
    "    export_models_for_deployment(trained_models)\n",
    "    \n",
    "    # Step 4: Load ensemble and test\n",
    "    print(\"\\nðŸ”„ Step 4: Loading and Testing Ensemble\")\n",
    "    ensemble = AutismGestureEnsemble(trained_models, CONFIG['GESTURE_NAMES'], threshold=0.5)\n",
    "    \n",
    "    # Test on sample videos (if available)\n",
    "    test_videos = glob.glob(os.path.join(CONFIG['DATA_FOLDER'], '*/*.avi'))[:10]  # First 10 videos\n",
    "    \n",
    "    if test_videos:\n",
    "        print(f\"ðŸ§ª Testing on {len(test_videos)} sample videos...\")\n",
    "        sample_predictions = ensemble.predict_single_video(test_videos[0])  # Only test first video\n",
    "        \n",
    "        print(f\"\\nðŸ“¹ Video 1: {os.path.basename(test_videos[0])}\")\n",
    "        if 'error' not in sample_predictions:\n",
    "            print(f\"   ðŸŽ¯ Detected: {sample_predictions['primary_gesture'] or 'None'}\")\n",
    "            print(f\"   ðŸ“Š Confidence: {float(sample_predictions['max_confidence']):.3f}\")\n",
    "            print(f\"   ðŸ“‹ All scores: {sample_predictions['all_confidences']}\")\n",
    "        else:\n",
    "            print(f\"   âŒ Error: {sample_predictions['error']}\")\n",
    "                \n",
    "    print(\"\\nâœ… Complete demo finished!\")\n",
    "    return ensemble, trained_models, training_histories, evaluation_results\n",
    "\n",
    "\n",
    "def test_single_video(video_path, ensemble=None, model_path='autism_gesture_ensemble.pth'):\n",
    "    \"\"\"\n",
    "    Test a single video with the trained ensemble model\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        ensemble: Pre-loaded ensemble model (optional)\n",
    "        model_path: Path to saved ensemble model\n",
    "    Returns:\n",
    "        Prediction result dictionary\n",
    "    \"\"\"\n",
    "    if ensemble is None:\n",
    "        print(\"ðŸ”„ Loading ensemble model...\")\n",
    "        ensemble = load_ensemble_model(model_path)\n",
    "        if ensemble is None:\n",
    "            return None\n",
    "\n",
    "    print(f\"ðŸ” Analyzing video: {video_path}\")\n",
    "    prediction = ensemble.predict_single_video(video_path)\n",
    "\n",
    "    print(\"\\nðŸ“Š Analysis Results:\")\n",
    "    if 'error' in prediction:\n",
    "        print(f\"âŒ Error: {prediction['error']}\")\n",
    "    else:\n",
    "        # Get primary gesture and confidences\n",
    "        primary_gesture = prediction['primary_gesture']\n",
    "        max_confidence = prediction['max_confidence']\n",
    "        all_confidences = prediction['all_confidences']\n",
    "\n",
    "        print(f\"ðŸŽ¯ Primary Gesture: {primary_gesture or 'None'}\")\n",
    "        print(f\"ðŸ“ˆ Confidence: {float(max_confidence):.3f}\")\n",
    "        print(\"\\nðŸ“‹ Detailed Confidences:\")\n",
    "\n",
    "        # Show individual gesture confidences\n",
    "        for gesture_name, confidence_dict in all_confidences.items():\n",
    "            confidence = confidence_dict.get('confidence', 0.0)\n",
    "            print(f\"  {gesture_name}: {float(confidence):.3f}\")\n",
    "\n",
    "        # New logic: Autism indication\n",
    "        autism_detected = any(\n",
    "            float(confidence_dict.get('confidence', 0.0)) > 0.5\n",
    "            for confidence_dict in all_confidences.values()\n",
    "        )\n",
    "        if autism_detected:\n",
    "            print(\"\\nâœ… Autism-related gesture detected!\")\n",
    "        else:\n",
    "            print(\"\\nðŸš« No autism-related gesture detected.\")\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def interactive_cli_interface(ensemble=None, model_path='autism_gesture_ensemble.pth'):\n",
    "    \"\"\"\n",
    "    Interactive command-line interface for using the autism gesture detection system\n",
    "    \"\"\"\n",
    "    print(\"ðŸ¤– Starting Autism Gesture Detection CLI Interface\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if ensemble is None:\n",
    "        print(\"ðŸ”„ Loading ensemble model...\")\n",
    "        ensemble = load_ensemble_model(model_path)\n",
    "        if ensemble is None:\n",
    "            return\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nMENU:\")\n",
    "        print(\"1. Test single video\")\n",
    "        print(\"2. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-2): \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            video_path = input(\"Enter video path: \")\n",
    "            if os.path.exists(video_path):\n",
    "                test_single_video(video_path, ensemble)\n",
    "            else:\n",
    "                print(\"âŒ File not found!\")\n",
    "                \n",
    "        elif choice == '2':\n",
    "            print(\"ðŸ‘‹ Exiting CLI interface. Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ Invalid choice! Please enter a number between 1 and 2.\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§  Welcome to Autism Gesture Detection System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Option 1: Run complete demo\n",
    "    # ensemble, trained_models, histories, results = run_complete_demo()\n",
    "    \n",
    "    # Option 2: Load existing model and start CLI\n",
    "    ensemble = load_ensemble_model()\n",
    "    \n",
    "    if ensemble:\n",
    "        interactive_cli_interface(ensemble)\n",
    "    else:\n",
    "        print(\"âŒ Failed to load ensemble model. Please check that model files exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
